[Server Options]
    #The port to listen to; <unsigned integer>
    server_port=9002

    #The flag indicating whether a TLS server is to be started for WSS protocol;
    is_tls_server=false

    #The number of threads to run for sentence translation
    num_threads=35

    #The source language name; <string>
    source_lang=Chinese

    #The target language name; <string>
    target_lang=English

[Language Models]
    #The language model file name; <string>
    lm_conn_string=./models/english.bitext.lm

    #The language model unknown word probability in the log_e space
    unk_word_log_e_prob=-10.7763748168945

    #The language model weight(s) used for tuning, are | separated
    lm_feature_weights=0.0645685337923864

[Translation Models]
    #The translation model file name; <string>
    tm_conn_string=./models/chinese.english.head.10.tm

    #The translation model weight(s) used for tuning.
    #There should be as many weights as there is features in
    #the translation models, with the same order, are | separated.
    #The meaning of the weights is as follows:
    #   weight[0] is for p(f|e);
    #   weight[1] is for lex(p(f|e));
    #   weight[2] is for p(e|f);
    #   weight[3] is for lex(p(e|f));
    #   weight[4] is for phrase_penalty;
    tm_feature_weights=0.0229820775641523|0.0465652763361929|0.0391134742902503|0.0472747582977873|0.19029115333165

    #Stores the unknown entry features, these should be
    #as is and will be put to log scale and multiplied
    #with tm_feature_weights by the tool, are | separated
    tm_unk_features=4.539992e-5|4.539992e-5|4.539992e-5|4.539992e-5|2.71828182846

    #Only consider the top 20 translations for each phrase only; <unsigned integer>
    #If the value is set to <= 2 then there is no limit.
    tm_trans_lim=30

    #Only consider the translations with minimum probability p(f|e) and p(e|f)
    #larger than this value; <unsigned float>, without using feature weights
    #tm_min_trans_prob=1e-5  #Oister did not use this limit in its experiments
    tm_min_trans_prob=0

    #Word penalty is a value given for each produced word; <float>:
    #     size(target_phrase_words) * word_penalty
    # < 0.0  we prefer longer translations (more words in the target sentence)
    # == 0.0 there is no word penalty
    # > 0.0  we prefer shorter translations (less words in the target sentence)
    tm_word_penalty=-0.226260664745573

[Reordering Models]
    #The reordering model file name; <string>
    rm_conn_string=./models/chinese.english.head.10.rm

    #The reordering model weight(s) used for tuning
    #There should be as many weights as there is features
    #in the translation models, with the same order,
    #are | separated
    rm_feature_weights=0.0453197307679924|0.0454568183558066|0.0917929511041461|0.093049083186734|0.0307707606230566|0.0378371259965423|0.1470201029395|0.0329432550781929

[Decoding Options]
    #The pruning threshold is to be a <unsigned float> it is
    #the %/100 deviation from the best hypothesis score.
    de_pruning_threshold=0.1

    #The stack capacity for stack pruning; <unsigned integer>
    de_stack_capacity=100

    #Stores the maximum considered source phrase length; <unsigned integer>
    de_max_source_phrase_length=7

    #Stores the maximum considered target phrase length,
    #should agree with the language and translation
    #servers; <unsigned integer>
    de_max_target_phrase_length=7

    #The distortion limit to use; <unsigned integer>
    #The the number of words to the right and left
    #from the last phrase end word to consider
    #A zero value means no distortion limit.
    #de_dist_lim=5
    #This is the limit as in Oister, we might run with another one for testing
    de_dist_lim=4

    #The lambda parameter to be used as a multiplier for the
    #linear distortion: <float>, can be positive or negative
    # > 0.0  is a linear distortion penalty
    # == 0.0 the linear distortion is not taken into account
    # < 0.0 is a linear distortion reward
    de_lin_dist_penalty=0.0143140771032327

    #The the tuning search lattice generation flag;
    #May be set to true only if the server is compiled with
    #the IS_SERVER_TUNING_MODE macro flag set to true.      
    de_is_gen_lattice=false

    #Stores the lattice data folder location for where the
    #generated lattice information is to be stored.
    de_lattices_folder=./lattices

    #The file name extention for the feature id to name mapping file
    #needed for tuning. The file will be generated if the lattice
    #generation is enabled. It will have the same name as the config
    #file plus this extention.
    de_lattice_id2name_file_ext=feature_id2name

    #The file name extention for the feature scores file needed for tuning.
    #The file will be generated if the lattice generation is enabled.
    #It will have the same name as the session id plus the translation
    #job id plus this extention.
    de_feature_scores_file_ext=feature_scores

    #The file name extention for the lattice file needed for tuning.
    #The file will be generated if the lattice generation is enabled.
    #It will have the same name as the session id plus the translation
    #job id plus this extention.
    de_lattice_file_ext=lattices
